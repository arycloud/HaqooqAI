Project Documentation: LLM-Powered Legal Assistant Agent
1. Introduction and Project Overview
This document provides comprehensive documentation for the development of an LLM-powered agent designed to function as a specialized legal assistant for Pakistani law. The primary objective of this project is to create an intelligent and reliable system capable of understanding natural language queries, retrieving pertinent information from a curated local legal knowledge base, and, when necessary, performing targeted web searches to provide accurate and sourced answers. This initiative aims to streamline legal information retrieval, reduce manual research overhead, and enhance the trustworthiness of legal insights.

2. Project Goals
The overarching goals of this project are to:

Develop a Specialized Legal Assistant: Create an AI agent focused on providing information related to Pakistani laws, ordinances, and legal affairs.

Implement Retrieval-Augmented Generation (RAG): Integrate a RAG pipeline to ground LLM responses in factual, external data, thereby reducing hallucination.

Enable Intelligent Tool Use: Empower the LLM to intelligently select and utilize appropriate tools (local document search, web search) based on query intent and information availability.

Ensure Answer Accuracy and Attribution: Provide precise answers supported by verifiable sources, clearly citing the origin of the information.

Manage Scope: Define clear boundaries for the agent's capabilities, allowing it to politely decline queries outside its specialization.

3. System Architecture and Project Structure
The system employs a modular architecture, leveraging LangChain's framework for orchestrating LLM interactions and tool utilization. The project code is organized into a logical directory structure to separate concerns and enhance maintainability.

3.1. Project Structure
Haq_ooq_RAG/
├── data/
│   ├── pakistan_laws_chunks.csv
│   ├── pakistan_laws_chunks_with_embeddings.csv
│   ├── pakistan_laws_raw.csv
│   ├── pakistan_laws_sectioned.csv
│   ├── pakistan_laws_semantic_sections.csv
│   └── chroma_db/
├── notebooks/
│   ├── data_embedding.ipynb
│   ├── data_preprocessing.ipynb
│   ├── rag_pipeline.ipynb
│   └── rag_with_tools.ipynb
├── src/
│   ├── __init__.py
│   ├── config.py
│   ├── data_processing/
│   │   ├── __init__.py
│   │   ├── preprocess.py
│   │   └── embed_and_index.py
│   └── agent/
│       ├── __init__.py
│       ├── tools.py
│       └── agent.py
├── .gitignore
├── README.md
└── requirements.txt

3.2. High-Level Diagram (Conceptual)
The system architecture remains the same, with the new file structure providing the concrete implementation for each component.

+----------------+       +-------------------+       +--------------------+
| User Query     | ----> | LLM Agent         | ----> | Generative LLM     |
+----------------+       | (src/agent/agent.py) |       | (Qwen3:1.7b)       |
                         |                   |       +--------------------+
                         | - Reasoning       |            ^
                         | - Tool Selection  |            |
                         | - Response        |            | Formulated Answer
                         |   Synthesis       |            |
                         +-------------------+            |
                                 |                        |
                                 | Tool Calls             |
                                 v                        |
                      +------------------+     +-----------------------+
                      | Tool Router      |     | Prompt Template       |
                      | (Agent Executor) |     | (src/agent/agent.py)  |
                      +------------------+     +-----------------------+
                               |
       +-------------------------------------------------+
       |                                                 |
       v                                                 v
+------------------------+                     +------------------------+
| Legal Document Search  |                     | Web Search (DuckDuckGo)|
| (src/agent/tools.py)   |                     | (for dynamic/external  |
+------------------------+                     |  legal info)           |
          ^                                    +------------------------+
          | Relevant Chunks
          |
+------------------------+
| Vector Database        |
| (data/chroma_db)       |
| - Legal Document Embeddings |
+------------------------+

3.3. Component Breakdown
Generative LLM (Qwen3:1.7b via Ollama): The core intelligence, configured in src/agent/agent.py.

Agent Executor: The orchestrator, also configured in src/agent/agent.py.

Tools: Defined in src/agent/tools.py. This includes the legal_document_search for local queries and the web_search for dynamic information.

Vector Database (ChromaDB): Persistent client path is configured in src/config.py, with the actual data stored in data/chroma_db.

Embedding Model (BAAI/bge-large-en-v1.5): Used for both embedding data chunks and user queries, with the model name specified in src/config.py.

4. Data Pipeline (Implementation)
The data pipeline has been refactored into two dedicated Python scripts for modularity.

src/data_processing/preprocess.py:

Functionality: This script handles all data preparation steps. It loads the raw data from data/pakistan_laws_raw.csv, cleans the text, applies semantic sectioning, and performs chunking with metadata preservation.

Output: It generates a data/pakistan_laws_chunks.csv file containing all the final text chunks ready for embedding.

src/data_processing/embed_and_index.py:

Functionality: This script reads the preprocessed chunks from data/pakistan_laws_chunks.csv, generates embeddings using the specified embedding model, and indexes them into the ChromaDB collection at data/chroma_db/.

Output: It saves a copy of the chunks with their embeddings to data/pakistan_laws_chunks_with_embeddings.csv and populates the persistent ChromaDB instance.

5. RAG Agent Implementation
The core logic for the RAG agent has been cleanly separated into two modules within the src/agent directory.

src/agent/tools.py:

Functionality: This module is responsible for setting up and initializing the ChromaDB client and embedding model for query-time retrieval. It defines the retrieve_relevant_chunks function and wraps it within a LangChain Tool, legal_document_search. It also defines the web_search_tool and assembles them into a list.

src/agent/agent.py:

Functionality: This is the heart of the agent. It initializes the ChatOllama LLM, defines the detailed system prompt with strict execution guidelines, binds the tools from src/agent/tools.py to the LLM, and creates the AgentExecutor that orchestrates the entire reasoning and tool-calling process.

6. Setup and Running the Project
Follow these steps to set up the project and run the agent in your local environment.

6.1. Prerequisites
Python 3.8+: Ensure you have a compatible Python version installed.

Ollama: Install Ollama from ollama.com/download.

LLM Model: Download the required LLM model by running the following command in your terminal. This needs to be done only once.

ollama pull qwen3:1.7b

6.2. Installation
Clone the Repository:

git clone https://github.com/your-username/Haq_ooq_RAG.git
cd Haq_ooq_RAG

Create a Virtual Environment: (Recommended)

python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate

Install Dependencies:

pip install -r requirements.txt

Place Raw Data: Ensure your pakistan_laws_raw.csv file is located in the data/ directory.

6.3. Running the Pipeline and Agent
The entire process, from data preparation to agent interaction, is automated through the main.py script.

Run the Main Script as a Module: Open your terminal and navigate to the project's root directory (Haq_ooq_RAG). Execute the script using the -m flag, which is the recommended way to run files within a package.

python -m src.main

Interactive Session: The script will first run the data preprocessing and indexing steps. Once complete, it will automatically launch an interactive command-line interface where you can type your questions to the legal assistant.

7. Key Features and Future Enhancements
The refactored project retains all the key features and capabilities of the original design, including contextual information retrieval, intelligent tool routing, and factual grounding.

The modular structure now paves the way for future enhancements with greater ease:

Web/Mobile Application Development: Building a user-facing application using frameworks like FastAPI/Flask for the backend and a modern frontend framework will be seamless.

Deployment: The new structure is perfectly suited for containerization with Docker and deployment on cloud platforms like Google Cloud Run or AWS App Runner.

Scalability: Separate modules allow for easier scaling of individual components, such as replacing the local ChromaDB with a cloud-managed vector store.

Advanced Tooling: New tools or data sources can be added to src/agent/tools.py without affecting other parts of the system.